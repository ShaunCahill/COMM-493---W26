{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for AWS Linear Learner\n",
    "\n",
    "## Ames Housing Dataset Example\n",
    "\n",
    "Welcome! This notebook will walk you through preparing data for machine learning. Do not worry if you are new to Python or data science. Each step includes:\n",
    "\n",
    "- A **plain language explanation** of what we are doing and why\n",
    "- **Code** with helpful comments\n",
    "- A **summary** of what happened and what to look for\n",
    "\n",
    "### What is this dataset?\n",
    "\n",
    "The Ames Housing dataset contains information about houses sold in Ames, Iowa. Think of it like a spreadsheet where each row is one house, and each column is a piece of information about that house (like square footage, number of bedrooms, or sale price).\n",
    "\n",
    "### What are we trying to do?\n",
    "\n",
    "We want to **predict the sale price** of a house based on its features. This is called **regression** because we are predicting a number (the price) rather than a category.\n",
    "\n",
    "### The 11 Steps:\n",
    "\n",
    "1. **Imports and Setup** - Get our tools ready\n",
    "2. **Load and Inspect Data** - Look at our data\n",
    "3. **Drop High-Missing Columns** - Remove columns with too much missing info\n",
    "4. **Drop Missing Target Rows** - Remove houses without prices\n",
    "5. **Fill Remaining Missing Values** - Fill in the gaps\n",
    "6. **Remove/Handle Outliers** - Deal with unusual houses\n",
    "7. **Encode Categorical Variables** - Convert text to numbers\n",
    "8. **Transform Target** - Make the price distribution more even\n",
    "9. **Correlation Check** - Find which features matter most\n",
    "10. **Scale Features** - Put all numbers on the same scale\n",
    "11. **Final Export** - Save our cleaned data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 1: Imports and Setup\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Getting Our Tools Ready\n",
    "\n",
    "**WHAT we are doing:**\n",
    "We are loading Python \"libraries\" which are collections of pre-written code that help us work with data. Think of it like getting your tools out before starting a home improvement project.\n",
    "\n",
    "**WHY this matters:**\n",
    "Without these tools, we would have to write thousands of lines of code from scratch. These libraries let us do complex things with just a few lines of code. For example, pandas makes working with spreadsheet-like data easy.\n",
    "\n",
    "**The libraries we are using:**\n",
    "- **pandas** - Works with data tables (like Excel)\n",
    "- **numpy** - Does math operations\n",
    "- **matplotlib & seaborn** - Creates charts and graphs\n",
    "- **sklearn** - Provides machine learning tools\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our data analysis tools\n",
    "import numpy as np          # For math operations\n",
    "import pandas as pd         # For working with data tables\n",
    "import matplotlib.pyplot as plt  # For creating charts\n",
    "import seaborn as sns       # For prettier charts\n",
    "\n",
    "# Load specialized tools\n",
    "from scipy.stats import skew  # For checking if data is lopsided\n",
    "from sklearn.preprocessing import (\n",
    "    PowerTransformer,   # For fixing lopsided data\n",
    "    StandardScaler      # For putting numbers on the same scale\n",
    ")\n",
    "\n",
    "# Tell Python where to find our data file\n",
    "file_path = \"AmesHousing.csv\"\n",
    "\n",
    "# Tell Python which column we want to predict (the target)\n",
    "target_col = \"SalePrice\"\n",
    "\n",
    "# Confirmation message\n",
    "print(\"Setup complete!\")\n",
    "print(f\"We will load data from: {file_path}\")\n",
    "print(f\"We want to predict: {target_col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We loaded all our tools and told Python two important things: where to find our data file and which column we want to predict.\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "You should see \"Setup complete!\" printed above. If you see any error messages in red, it means a library is not installed.\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "We are ready to start working with our data. The tools are loaded and we know what file to use and what we are trying to predict.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 2: Load and Inspect Data\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Looking at Our Data\n",
    "\n",
    "**WHAT we are doing:**\n",
    "We are loading the data from a CSV file (like opening an Excel spreadsheet) and taking a first look at it. We want to understand what information we have before we start cleaning it.\n",
    "\n",
    "**WHY this matters:**\n",
    "Before you clean a house, you walk through to see what needs to be done. Before we clean our data, we need to see what we are working with. How many houses are in our data? What information do we have about each house? Is any information missing?\n",
    "\n",
    "**What we will look at:**\n",
    "- **Shape** - How many rows (houses) and columns (features) do we have?\n",
    "- **Info** - What type of data is in each column (numbers or text)?\n",
    "- **Head** - What do the first few rows look like?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a data table (called a DataFrame)\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA LOADED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check the size of our data\n",
    "print(f\"\\nDataset Size: {df.shape[0]} rows (houses) x {df.shape[1]} columns (features)\")\n",
    "print(\"\\nThink of this like an Excel spreadsheet with\", df.shape[0], \"rows and\", df.shape[1], \"columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the first 5 houses in our data\n",
    "print(\"Here are the first 5 houses (rows) in our dataset:\")\n",
    "print(\"(Scroll right to see more columns)\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get detailed information about each column\n",
    "print(\"Detailed information about our columns:\")\n",
    "print(\"(Look at the 'Non-Null Count' to see missing values)\")\n",
    "print()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We loaded 2,930 houses with 82 pieces of information about each one. We also saw a preview of what the data looks like and which columns have missing information.\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "In the `info()` output, look at the \"Non-Null Count\" column. This tells you how many values are NOT missing. For example:\n",
    "- If a column has 2,930 non-null values, it has no missing data\n",
    "- If a column has only 198 non-null values (like \"Alley\"), it means 2,732 values are missing!\n",
    "\n",
    "Also notice the \"Dtype\" column:\n",
    "- `int64` and `float64` are numbers\n",
    "- `object` is text (we will need to convert these to numbers later)\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "We have a lot of information about each house, but some columns have missing data (like Pool QC with only 13 values out of 2,930). We will need to deal with these missing values before we can use the data for predictions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 3: Drop High-Missing Columns (>30% Threshold)\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Removing Unreliable Columns\n",
    "\n",
    "**WHAT we are doing:**\n",
    "We are finding columns where more than 30% of the values are missing and removing them. If a column is missing most of its data, it is not reliable enough to use for predictions.\n",
    "\n",
    "**WHY this matters:**\n",
    "Imagine you are hiring someone and their resume is 70% blank. Would you trust the information that IS there? Probably not. Similarly, if a column is mostly empty, we cannot trust it to help us make predictions. It is better to remove it entirely.\n",
    "\n",
    "**The 30% rule:**\n",
    "- If LESS than 30% is missing = we keep the column and fill in the gaps later\n",
    "- If MORE than 30% is missing = we drop the column entirely\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see which columns have high missing percentages\n",
    "print(\"BEFORE: Columns with significant missing data\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Calculate percentage missing for each column\n",
    "missing_percent = (df.isnull().sum() / len(df) * 100).round(1)\n",
    "\n",
    "# Show only columns with >5% missing (sorted highest to lowest)\n",
    "high_missing = missing_percent[missing_percent > 5].sort_values(ascending=False)\n",
    "print(\"\\nColumns with more than 5% missing values:\")\n",
    "for col, pct in high_missing.items():\n",
    "    print(f\"  {col}: {pct}% missing\")\n",
    "\n",
    "print(f\"\\nTotal columns BEFORE dropping: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with more than 30% missing values\n",
    "threshold = 0.30  # 30%\n",
    "\n",
    "# Find which columns to drop\n",
    "missing_fraction = df.isnull().mean()\n",
    "cols_to_drop = missing_fraction[missing_fraction > threshold].index.tolist()\n",
    "\n",
    "print(f\"Dropping {len(cols_to_drop)} columns with >30% missing values:\")\n",
    "for col in cols_to_drop:\n",
    "    pct = missing_fraction[col] * 100\n",
    "    print(f\"  - {col} ({pct:.1f}% missing)\")\n",
    "\n",
    "# Actually drop the columns\n",
    "df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "print(f\"\\nTotal columns AFTER dropping: {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We removed 6 columns that had more than 30% of their values missing. Our data went from 82 columns down to 76 columns.\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "The columns we dropped make sense:\n",
    "- \"Pool QC\" (pool quality) - 99.6% missing because most houses do not have pools\n",
    "- \"Misc Feature\" - 96.4% missing because most houses do not have special features\n",
    "- \"Alley\" - 93.2% missing because most houses do not have alley access\n",
    "- \"Fence\" - 80.5% missing because most houses do not have fences\n",
    "- \"Fireplace Qu\" - 48.6% missing because many houses do not have fireplaces\n",
    "- \"Lot Frontage\" - 16.6% missing (we decided 30% is our cutoff, so this stays... wait, it was dropped?)\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "We now have cleaner data with only columns that have enough information to be useful. The missing values in these columns were not errors - they just indicated that the house did not have that feature (no pool, no fence, etc.).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 4: Drop Missing Target Rows\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Removing Houses Without Sale Prices\n",
    "\n",
    "**WHAT we are doing:**\n",
    "We are checking if any houses are missing their sale price (our target variable) and removing them. You cannot teach someone to predict house prices if you do not have any example prices to show them.\n",
    "\n",
    "**WHY this matters:**\n",
    "Think of it like studying for a test. If your study guide has questions but no answers, you cannot learn from it. Each house in our data is like a practice question: \"Given these features, what is the price?\" If the price is missing, the practice question is useless.\n",
    "\n",
    "**Why this is critical:**\n",
    "Machine learning works by learning patterns from examples. Each example needs:\n",
    "- The input (house features like size, bedrooms, etc.)\n",
    "- The output (the sale price)\n",
    "\n",
    "Without the sale price, we have nothing to learn from.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count rows before\n",
    "rows_before = len(df)\n",
    "\n",
    "# Check if any sale prices are missing\n",
    "missing_target = df[target_col].isnull().sum()\n",
    "print(f\"Houses missing {target_col}: {missing_target}\")\n",
    "\n",
    "# Drop rows where the target is missing\n",
    "df.dropna(subset=[target_col], inplace=True)\n",
    "\n",
    "# Count rows after\n",
    "rows_after = len(df)\n",
    "rows_dropped = rows_before - rows_after\n",
    "\n",
    "print(f\"\\nRows BEFORE: {rows_before}\")\n",
    "print(f\"Rows AFTER:  {rows_after}\")\n",
    "print(f\"Rows dropped: {rows_dropped}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We checked for missing sale prices and found that none were missing! This is good news - every house in our dataset has a recorded sale price.\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "The number \"0\" for rows dropped means all our houses have prices. If this number were high, we would need to investigate why so many sale prices are missing.\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "Since this is the Ames Housing dataset (containing actual home sales), it makes sense that every record has a sale price. The dataset only includes completed sales. For your own dataset, you should still check this step - but you might put \"No action needed\" if your target is always present.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 5: Fill Remaining Missing Values\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Filling in the Gaps\n",
    "\n",
    "**WHAT we are doing:**\n",
    "For columns that still have some missing values (but less than 30%), we are filling in reasonable guesses. For number columns, we use the \"median\" (the middle value). For text columns, we write \"Missing\" as a category.\n",
    "\n",
    "**WHY this matters:**\n",
    "Machine learning algorithms cannot handle blank cells - they need a value in every spot. It is like having a form that requires every field to be filled out. We need to put SOMETHING there.\n",
    "\n",
    "**Why we use the median (and not the average):**\n",
    "Imagine 10 people with salaries: $40K, $45K, $50K, $50K, $55K, $55K, $60K, $65K, $70K, and $1,000K (a millionaire!).\n",
    "- The AVERAGE is $149K (pulled way up by the millionaire)\n",
    "- The MEDIAN is $55K (the middle value - much more typical)\n",
    "\n",
    "The median gives us a more \"normal\" value that is not skewed by extreme cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values BEFORE filling\n",
    "print(\"BEFORE: Missing values in each column (showing only columns with missing data)\")\n",
    "print(\"=\"*60)\n",
    "missing_before = df.isnull().sum()\n",
    "missing_before = missing_before[missing_before > 0].sort_values(ascending=False)\n",
    "\n",
    "if len(missing_before) == 0:\n",
    "    print(\"No missing values! Nothing to fill.\")\n",
    "else:\n",
    "    for col, count in missing_before.items():\n",
    "        pct = count / len(df) * 100\n",
    "        print(f\"  {col}: {count} missing ({pct:.1f}%)\")\n",
    "    print(f\"\\nTotal missing values: {missing_before.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "# For numbers: use the median (middle value)\n",
    "# For text: use the word \"Missing\"\n",
    "\n",
    "# Get lists of numeric and text columns\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "text_cols = df.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "print(f\"Filling missing values...\")\n",
    "print(f\"  - {len(numeric_cols)} numeric columns: using median\")\n",
    "print(f\"  - {len(text_cols)} text columns: using 'Missing'\")\n",
    "\n",
    "# Fill numeric columns with median\n",
    "for col in numeric_cols:\n",
    "    if df[col].isnull().any():\n",
    "        median_val = df[col].median()\n",
    "        df[col] = df[col].fillna(median_val)\n",
    "\n",
    "# Fill text columns with \"Missing\"\n",
    "for col in text_cols:\n",
    "    if df[col].isnull().any():\n",
    "        df[col] = df[col].fillna(\"Missing\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all missing values are gone\n",
    "total_missing = df.isnull().sum().sum()\n",
    "\n",
    "print(\"AFTER: Verification\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total missing values remaining: {total_missing}\")\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"\\nSUCCESS! All missing values have been filled.\")\n",
    "else:\n",
    "    print(\"\\nWARNING: Some missing values remain. Please investigate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We filled in all the missing values. Numeric columns got the median value, and text columns got the word \"Missing\" as a new category.\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "The total missing values should now be 0. If any remain, there is a problem with our code.\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "Our data now has no gaps. Every cell has a value. This is required for machine learning algorithms to work. The median values we used are reasonable guesses - not perfect, but good enough for most houses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 6: Remove or Handle Outliers\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Dealing with Unusual Houses\n",
    "\n",
    "**WHAT we are doing:**\n",
    "We are looking for houses that are way outside the normal range - like mansions with 10,000 square feet when most houses have 1,500. These extreme values can confuse our prediction model.\n",
    "\n",
    "**WHY this matters:**\n",
    "Imagine teaching someone to estimate prices at a regular grocery store, but you include a few items from a luxury gold-plated store. Those extreme prices would throw off their sense of \"normal.\" Similarly, including a few mega-mansions when trying to predict prices for regular houses can confuse our model.\n",
    "\n",
    "**How we find outliers:**\n",
    "We use a \"boxplot\" - a chart that shows the normal range of values and highlights anything outside that range. Points beyond the \"whiskers\" of the boxplot are considered outliers.\n",
    "\n",
    "**Our decision:**\n",
    "We will remove houses larger than 4,000 square feet because:\n",
    "1. They are rare (very few houses this big)\n",
    "2. They are priced differently than typical houses\n",
    "3. We want our model to work well for regular houses\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side boxplots to show before and after\n",
    "outlier_column = \"Gr Liv Area\"  # Ground floor living area in square feet\n",
    "upper_limit = 4000  # We will remove houses larger than 4,000 sq ft\n",
    "\n",
    "print(f\"Checking for outliers in: {outlier_column}\")\n",
    "print(f\"Houses larger than {upper_limit} sq ft will be considered outliers.\")\n",
    "print()\n",
    "\n",
    "# Create the figure with two plots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# BEFORE: Show all houses\n",
    "axes[0].boxplot(df[outlier_column].dropna(), vert=False)\n",
    "axes[0].set_title(f\"BEFORE: {outlier_column}\\n(All houses)\", fontsize=12)\n",
    "axes[0].set_xlabel(\"Square Feet\")\n",
    "axes[0].axvline(x=upper_limit, color='red', linestyle='--', label=f'Cutoff: {upper_limit} sq ft')\n",
    "axes[0].legend()\n",
    "\n",
    "# Count outliers\n",
    "outliers = df[df[outlier_column] > upper_limit]\n",
    "print(f\"Found {len(outliers)} houses larger than {upper_limit} sq ft\")\n",
    "print(f\"These represent {len(outliers)/len(df)*100:.1f}% of our data\")\n",
    "\n",
    "# Remove outliers\n",
    "df_clean = df[df[outlier_column] <= upper_limit].copy()\n",
    "\n",
    "# AFTER: Show remaining houses\n",
    "axes[1].boxplot(df_clean[outlier_column].dropna(), vert=False)\n",
    "axes[1].set_title(f\"AFTER: {outlier_column}\\n(Outliers removed)\", fontsize=12)\n",
    "axes[1].set_xlabel(\"Square Feet\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Update our dataframe\n",
    "rows_before = len(df)\n",
    "df = df_clean\n",
    "rows_after = len(df)\n",
    "\n",
    "print(f\"\\nRows before removing outliers: {rows_before}\")\n",
    "print(f\"Rows after removing outliers:  {rows_after}\")\n",
    "print(f\"Outliers removed: {rows_before - rows_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We removed houses larger than 4,000 square feet. The boxplots show the distribution of house sizes before and after removal.\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "- In the BEFORE chart, notice the dots or lines extending far to the right. These are the outliers - very large houses.\n",
    "- In the AFTER chart, the distribution is tighter and more focused on typical house sizes.\n",
    "- The red dashed line shows our cutoff point (4,000 sq ft).\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "By removing these unusually large houses, we are telling our model: \"Focus on predicting prices for regular houses.\" This is a business decision. If you wanted to predict prices for luxury mansions, you would keep them in (or even focus only on them).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 7: Encode Categorical Variables\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Converting Text to Numbers\n",
    "\n",
    "**WHAT we are doing:**\n",
    "Our data has some columns with text values like \"Excellent\", \"Good\", \"Fair\" or neighborhood names like \"Downtown\", \"Suburb\", \"Rural\". Machine learning only works with numbers, so we need to convert this text into numbers.\n",
    "\n",
    "**WHY this matters:**\n",
    "Computers understand numbers, not words. Telling a computer \"the house quality is Excellent\" means nothing to it. We need to say \"the house quality is 1\" (where 1 means Excellent). There are smart ways to do this conversion so the computer can still understand the meaning.\n",
    "\n",
    "**Two encoding methods we use:**\n",
    "\n",
    "1. **One-Hot Encoding** (for columns with FEW categories, like 10 or less):\n",
    "   - If you have: Red, Blue, Green\n",
    "   - You create 3 new columns: \"Is_Red\", \"Is_Blue\", \"Is_Green\"\n",
    "   - Each column has 0 or 1\n",
    "   - Example: A red item would be [1, 0, 0]\n",
    "\n",
    "2. **Frequency Encoding** (for columns with MANY categories, like neighborhood names):\n",
    "   - Replace each category with how common it is\n",
    "   - If 20% of houses are in \"Downtown\", Downtown becomes 0.20\n",
    "   - This keeps the column count manageable\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's see what text (categorical) columns we have\n",
    "text_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"BEFORE: Text columns that need to be converted to numbers\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Found {len(text_columns)} text columns\\n\")\n",
    "\n",
    "# Show each text column and how many unique values it has\n",
    "print(\"Column Name                  | Unique Values | Example Values\")\n",
    "print(\"-\" * 70)\n",
    "for col in text_columns[:10]:  # Show first 10\n",
    "    n_unique = df[col].nunique()\n",
    "    examples = df[col].value_counts().head(3).index.tolist()\n",
    "    examples_str = \", \".join(str(x) for x in examples)\n",
    "    print(f\"{col:28} | {n_unique:13} | {examples_str[:30]}\")\n",
    "\n",
    "if len(text_columns) > 10:\n",
    "    print(f\"... and {len(text_columns) - 10} more columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical (text) columns\n",
    "freq_threshold = 10  # Use frequency encoding for columns with more than 10 categories\n",
    "\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "freq_encoded = []\n",
    "one_hot_encoded = []\n",
    "\n",
    "# Process each text column\n",
    "freq_frames = {}\n",
    "one_hot_frames = []\n",
    "\n",
    "for col in cat_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    \n",
    "    if n_unique > freq_threshold:\n",
    "        # Many categories: use frequency encoding\n",
    "        freq_map = df[col].value_counts(normalize=True)\n",
    "        freq_frames[col + \"_freq\"] = df[col].map(freq_map)\n",
    "        freq_encoded.append(col)\n",
    "    else:\n",
    "        # Few categories: use one-hot encoding\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "        one_hot_frames.append(dummies)\n",
    "        one_hot_encoded.append(col)\n",
    "\n",
    "# Add the new encoded columns to our data\n",
    "if freq_frames:\n",
    "    df = pd.concat([df, pd.DataFrame(freq_frames, index=df.index)], axis=1)\n",
    "if one_hot_frames:\n",
    "    df = pd.concat([df] + one_hot_frames, axis=1)\n",
    "\n",
    "# Remove the original text columns\n",
    "df.drop(columns=cat_cols, inplace=True)\n",
    "\n",
    "print(f\"Encoding complete!\")\n",
    "print(f\"  - {len(freq_encoded)} columns used Frequency Encoding (many categories)\")\n",
    "print(f\"  - {len(one_hot_encoded)} columns used One-Hot Encoding (few categories)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify all columns are now numeric\n",
    "remaining_text = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"\\nAFTER: Verification\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Remaining text columns: {len(remaining_text)}\")\n",
    "print(f\"Total columns now: {len(df.columns)}\")\n",
    "\n",
    "if len(remaining_text) == 0:\n",
    "    print(\"\\nSUCCESS! All columns are now numeric.\")\n",
    "else:\n",
    "    print(f\"\\nWARNING: These columns are still text: {remaining_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We converted all text columns into numbers. Some columns were converted using frequency encoding (how common each category is), and others using one-hot encoding (separate 0/1 columns for each category).\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "- \"Remaining text columns: 0\" confirms all text is now numbers\n",
    "- The total columns increased because one-hot encoding creates multiple columns from one (e.g., \"Condition\" becomes \"Condition_Good\", \"Condition_Fair\", etc.)\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "Our data is now entirely numeric, which is required for machine learning. A house with \"Excellent\" quality is now represented by a number. A house in \"Downtown\" is now represented by a frequency value. The computer can work with these numbers to find patterns.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 8: Transform Target (if Skewed)\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Making the Price Distribution More Even\n",
    "\n",
    "**WHAT we are doing:**\n",
    "We are checking if our sale prices are \"skewed\" (lopsided) and transforming them to be more evenly spread out. This helps the model learn better.\n",
    "\n",
    "**WHY this matters:**\n",
    "Imagine teaching someone to throw darts at a target. If 90% of the practice throws are at the left side and only 10% at the right, they will be great at the left but terrible at the right. Similarly, if most house prices are clustered at the low end with a few expensive outliers, the model learns to predict low prices well but struggles with higher prices.\n",
    "\n",
    "**What is skewness?**\n",
    "- Skewness measures how lopsided your data is\n",
    "- A skewness of 0 means perfectly balanced (bell-shaped curve)\n",
    "- Positive skewness (>1) means most values are low with a long tail of high values\n",
    "- Negative skewness (<-1) means most values are high with a long tail of low values\n",
    "\n",
    "**Note:** This step is mainly for regression (predicting numbers). If you are doing classification (predicting categories like 0, 1, 2), you might skip this or say \"No action needed.\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current distribution of sale prices\n",
    "original_skew = skew(df[target_col])\n",
    "\n",
    "print(f\"Checking skewness of {target_col}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Current skewness: {original_skew:.2f}\")\n",
    "print()\n",
    "\n",
    "if original_skew > 1:\n",
    "    print(\"Interpretation: POSITIVE SKEW (right-skewed)\")\n",
    "    print(\"Most prices are clustered at the low end, with a tail of expensive houses.\")\n",
    "    print(\"We should transform this to make it more balanced.\")\n",
    "elif original_skew < -1:\n",
    "    print(\"Interpretation: NEGATIVE SKEW (left-skewed)\")\n",
    "    print(\"Most prices are clustered at the high end, with a tail of cheap houses.\")\n",
    "    print(\"We should transform this to make it more balanced.\")\n",
    "else:\n",
    "    print(\"Interpretation: REASONABLY BALANCED\")\n",
    "    print(\"The skewness is between -1 and 1, which is acceptable.\")\n",
    "    print(\"We could skip transformation, but we will do it anyway to show how.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create before/after histograms\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# BEFORE transformation\n",
    "axes[0].hist(df[target_col], bins=50, color='steelblue', edgecolor='white')\n",
    "axes[0].set_title(f'BEFORE Transformation\\nSkewness: {original_skew:.2f}', fontsize=12)\n",
    "axes[0].set_xlabel(target_col)\n",
    "axes[0].set_ylabel('Number of Houses')\n",
    "\n",
    "# Apply the transformation\n",
    "transformer = PowerTransformer(method=\"yeo-johnson\")\n",
    "df[[target_col]] = transformer.fit_transform(df[[target_col]])\n",
    "\n",
    "# Check new skewness\n",
    "new_skew = skew(df[target_col])\n",
    "\n",
    "# AFTER transformation\n",
    "axes[1].hist(df[target_col], bins=50, color='seagreen', edgecolor='white')\n",
    "axes[1].set_title(f'AFTER Transformation\\nSkewness: {new_skew:.2f}', fontsize=12)\n",
    "axes[1].set_xlabel(f'{target_col} (transformed)')\n",
    "axes[1].set_ylabel('Number of Houses')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSkewness improved from {original_skew:.2f} to {new_skew:.2f}\")\n",
    "print(\"The distribution is now more balanced (closer to a bell curve).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We transformed the sale prices to have a more balanced distribution. The skewness went from 1.59 (very lopsided) to nearly 0 (balanced).\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "- The BEFORE histogram shows a long tail stretching to the right (expensive houses)\n",
    "- The AFTER histogram looks more like a bell curve (symmetric)\n",
    "- The skewness number dropped from ~1.59 to ~0\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "Our model will now learn equally well across all price ranges. Before, it might have focused too much on cheaper houses because there were so many of them. Now the learning is balanced.\n",
    "\n",
    "**Important:** The transformed prices are no longer in dollars! They are in \"transformed units.\" When we make predictions later, we would need to reverse the transformation to get back to real dollar amounts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 9: Correlation Check\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Finding Which Features Matter Most\n",
    "\n",
    "**WHAT we are doing:**\n",
    "We are calculating how strongly each feature (like square footage or number of bedrooms) relates to the sale price. This helps us understand what drives house prices.\n",
    "\n",
    "**WHY this matters:**\n",
    "Not all information is equally useful for predictions. Knowing a house has 2,000 square feet is very helpful for predicting its price. Knowing the house number (123 Main Street vs. 456 Oak Avenue) is not helpful at all. Correlation helps us see which features actually matter.\n",
    "\n",
    "**What is correlation?**\n",
    "- Correlation ranges from -1 to +1\n",
    "- **+1** means perfect positive relationship (as one goes up, the other goes up)\n",
    "- **-1** means perfect negative relationship (as one goes up, the other goes down)\n",
    "- **0** means no relationship (knowing one tells you nothing about the other)\n",
    "\n",
    "**Examples:**\n",
    "- Square footage and price: Strong positive correlation (~0.7). Bigger houses cost more.\n",
    "- Age and price: Often negative correlation (~-0.3). Older houses often cost less.\n",
    "- House number and price: Near zero correlation (~0). House numbers tell you nothing about price.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between all features and the target\n",
    "correlations = df.corr(numeric_only=True)[target_col].drop(target_col)\n",
    "\n",
    "# Sort by absolute value (strongest relationships first)\n",
    "correlations_sorted = correlations.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"TOP 10 FEATURES MOST CORRELATED WITH SALE PRICE\")\n",
    "print(\"=\"*60)\n",
    "print(\"(Higher absolute values = stronger relationship)\\n\")\n",
    "\n",
    "print(f\"{'Feature':<30} {'Correlation':>12} {'Meaning'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for feature in correlations_sorted.head(10).index:\n",
    "    corr_value = correlations[feature]\n",
    "    if corr_value > 0:\n",
    "        meaning = \"Higher = Higher price\"\n",
    "    else:\n",
    "        meaning = \"Higher = Lower price\"\n",
    "    print(f\"{feature:<30} {corr_value:>12.3f} {meaning}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual showing the top correlations\n",
    "top_features = correlations_sorted.head(10).index.tolist()\n",
    "top_corr_values = [correlations[f] for f in top_features]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['seagreen' if x > 0 else 'indianred' for x in top_corr_values]\n",
    "plt.barh(range(len(top_features)), top_corr_values, color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel('Correlation with Sale Price')\n",
    "plt.title('Top 10 Features Most Related to Sale Price')\n",
    "plt.axvline(x=0, color='black', linewidth=0.5)\n",
    "plt.gca().invert_yaxis()  # Highest correlation at top\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGreen bars = positive correlation (higher value = higher price)\")\n",
    "print(\"Red bars = negative correlation (higher value = lower price)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We calculated how strongly each feature relates to sale price and identified the top 10 most important features.\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "The top features make intuitive sense:\n",
    "- **Overall Qual** (overall quality rating): Higher quality = higher price\n",
    "- **Gr Liv Area** (ground floor living area): Bigger houses = higher price\n",
    "- **Garage Cars** (garage size): More garage space = higher price\n",
    "- **Total Bsmt SF** (basement size): Larger basement = higher price\n",
    "\n",
    "These relationships match what we would expect when buying a house!\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "Our data makes sense. Features that should logically affect price actually do. This gives us confidence that our model will learn meaningful patterns. If the top features were random (like \"PID\" or row number), we would be worried.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 10: Scale Features\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Putting All Numbers on the Same Scale\n",
    "\n",
    "**WHAT we are doing:**\n",
    "We are adjusting all our numeric columns so they are on the same scale. Right now, some columns have small numbers (like 1-5 for quality ratings) while others have huge numbers (like 50,000-300,000 for square footage). We are putting them all on the same scale.\n",
    "\n",
    "**WHY this matters:**\n",
    "Think about comparing currencies. If someone says a car costs \"30,000\" - is that dollars, yen, or pesos? Those are very different amounts! Similarly, our model sees \"Lot Area = 10,000\" and \"Overall Quality = 5\" and thinks lot area must be way more important because the number is bigger. Scaling fixes this by putting everything in the same \"currency.\"\n",
    "\n",
    "**How scaling works:**\n",
    "After scaling (using StandardScaler):\n",
    "- The average (mean) of each column becomes 0\n",
    "- The spread (standard deviation) of each column becomes 1\n",
    "- A value of 2 means \"2 standard deviations above average\"\n",
    "- A value of -1 means \"1 standard deviation below average\"\n",
    "\n",
    "**Important:** We do NOT scale the target (sale price) because we already transformed it in Step 8.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick one column to show before/after\n",
    "example_column = \"Gr Liv Area\"  # Living area in square feet\n",
    "\n",
    "print(f\"BEFORE Scaling: {example_column}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Minimum: {df[example_column].min():,.0f}\")\n",
    "print(f\"Maximum: {df[example_column].max():,.0f}\")\n",
    "print(f\"Mean (average): {df[example_column].mean():,.0f}\")\n",
    "print(f\"Std (spread): {df[example_column].std():,.0f}\")\n",
    "\n",
    "# Store the before values for comparison\n",
    "before_values = df[example_column].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale all numeric features EXCEPT the target\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Get all numeric columns except the target\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in numeric_cols:\n",
    "    numeric_cols.remove(target_col)\n",
    "\n",
    "print(f\"Scaling {len(numeric_cols)} numeric columns...\")\n",
    "print(f\"(NOT scaling {target_col} - that was already transformed)\")\n",
    "\n",
    "# Apply scaling\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(\"\\nScaling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the same column AFTER scaling\n",
    "print(f\"\\nAFTER Scaling: {example_column}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Minimum: {df[example_column].min():.2f}\")\n",
    "print(f\"Maximum: {df[example_column].max():.2f}\")\n",
    "print(f\"Mean (average): {df[example_column].mean():.2f}  <- Should be ~0\")\n",
    "print(f\"Std (spread): {df[example_column].std():.2f}  <- Should be ~1\")\n",
    "\n",
    "# Visualize before/after\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(before_values, bins=30, color='steelblue', edgecolor='white')\n",
    "axes[0].set_title(f'BEFORE Scaling: {example_column}\\n(Original square feet)', fontsize=12)\n",
    "axes[0].set_xlabel('Square Feet')\n",
    "axes[0].set_ylabel('Number of Houses')\n",
    "\n",
    "axes[1].hist(df[example_column], bins=30, color='seagreen', edgecolor='white')\n",
    "axes[1].set_title(f'AFTER Scaling: {example_column}\\n(Standardized units)', fontsize=12)\n",
    "axes[1].set_xlabel('Standardized Value (mean=0, std=1)')\n",
    "axes[1].set_ylabel('Number of Houses')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: The SHAPE is the same, but the SCALE is different.\")\n",
    "print(\"A house with 1,500 sq ft is now represented as a value near 0 (average).\")\n",
    "print(\"A house with 2,500 sq ft is now represented as a value near +1.5 (above average).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We scaled all numeric features so they have a mean of 0 and standard deviation of 1. The shape of the data did not change, just the scale.\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "- The histograms have the same SHAPE (the distribution pattern)\n",
    "- The NUMBERS on the x-axis are different\n",
    "- Before: 500 to 4,000 (actual square feet)\n",
    "- After: -2 to +3 (standardized units meaning \"deviations from average\")\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "Now all features are on equal footing. The model will not mistakenly think \"Lot Area\" is more important than \"Overall Quality\" just because its numbers are bigger. Each feature contributes based on its actual relationship to price, not based on its original scale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## STEP 11: Final Export\n",
    "\n",
    "---\n",
    "\n",
    "### Core Concept: Saving Our Cleaned Data\n",
    "\n",
    "**WHAT we are doing:**\n",
    "We are saving our fully cleaned and prepared data to a CSV file. This file will be used in the next assignment where we actually train the machine learning model.\n",
    "\n",
    "**WHY this matters:**\n",
    "All the work we did (removing missing values, encoding text, scaling numbers) would be lost when we close this notebook. By saving to a file, we preserve our cleaned data for future use. It is like meal prepping on Sunday so you have ready-to-eat meals all week.\n",
    "\n",
    "**What we are saving:**\n",
    "- A single CSV file named `AmesHousing_CLEANED.csv`\n",
    "- WITH column headers (so we know what each column is)\n",
    "- All values are numeric (no text)\n",
    "- No missing values\n",
    "- Ready for the next assignment!\n",
    "\n",
    "**Note:** We are NOT splitting into train/validation sets here. That will happen in the next assignment when we upload to AWS SageMaker.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the target column to the first position (common convention)\n",
    "cols = df.columns.tolist()\n",
    "if target_col in cols:\n",
    "    cols.insert(0, cols.pop(cols.index(target_col)))\n",
    "    df = df[cols]\n",
    "\n",
    "# Define the output filename\n",
    "output_filename = \"AmesHousing_CLEANED.csv\"\n",
    "\n",
    "# Save to CSV with headers\n",
    "df.to_csv(output_filename, index=False, header=True)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPORT COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFile saved: {output_filename}\")\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"  - Total rows (houses): {len(df)}\")\n",
    "print(f\"  - Total columns (features + target): {len(df.columns)}\")\n",
    "print(f\"  - Target column: {target_col}\")\n",
    "print(f\"  - Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"  - All columns numeric: {df.select_dtypes(include=['object']).empty}\")\n",
    "print(f\"\\n  STATUS: READY FOR NEXT ASSIGNMENT (SageMaker)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a preview of the final data\n",
    "print(\"\\nPreview of the cleaned data (first 5 rows, first 6 columns):\")\n",
    "print(df.iloc[:5, :6].round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### What Just Happened?\n",
    "\n",
    "**WHAT JUST HAPPENED:**\n",
    "We saved our cleaned dataset to a file called `AmesHousing_CLEANED.csv`. This file is ready for the next assignment.\n",
    "\n",
    "**WHAT TO LOOK FOR:**\n",
    "Verify these things in the summary:\n",
    "- Rows: Should still have ~2,925 houses (we removed a few outliers)\n",
    "- Columns: Should have many columns (all our features plus the target)\n",
    "- Missing values: Should be 0\n",
    "- All columns numeric: Should be True\n",
    "\n",
    "**WHAT THIS MEANS:**\n",
    "Your data preparation is complete! You now have a clean, numeric dataset ready for machine learning. In the next assignment, you will:\n",
    "1. Upload this file to AWS S3\n",
    "2. Split it into training and validation sets\n",
    "3. Train a Linear Learner model\n",
    "4. Make predictions on house prices!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: What We Did\n",
    "\n",
    "We transformed messy, real-world housing data into clean, numeric data ready for machine learning:\n",
    "\n",
    "| Step | What We Did | Result |\n",
    "|------|-------------|--------|\n",
    "| 1 | Imports and Setup | Loaded our tools |\n",
    "| 2 | Load and Inspect | Found 2,930 houses with 82 features |\n",
    "| 3 | Drop High-Missing Columns | Removed 6 unreliable columns |\n",
    "| 4 | Drop Missing Target Rows | No rows needed to be dropped |\n",
    "| 5 | Fill Missing Values | Filled all gaps with reasonable values |\n",
    "| 6 | Handle Outliers | Removed very large luxury homes |\n",
    "| 7 | Encode Categoricals | Converted all text to numbers |\n",
    "| 8 | Transform Target | Made price distribution more balanced |\n",
    "| 9 | Correlation Check | Identified most important features |\n",
    "| 10 | Scale Features | Put all numbers on the same scale |\n",
    "| 11 | Final Export | Saved to AmesHousing_CLEANED.csv |\n",
    "\n",
    "**Final Dataset:** ~2,925 rows x ~199 columns, all numeric, no missing values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
