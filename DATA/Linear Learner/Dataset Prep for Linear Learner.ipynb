{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for AWS SageMaker Linear Learner\n",
    "\n",
    "This notebook prepares any CSV dataset for use with AWS SageMaker's Linear Learner algorithm.\n",
    "\n",
    "**You only need to change two things:**\n",
    "1. The filename of your dataset\n",
    "2. The name of your target column (what you want to predict)\n",
    "\n",
    "Run each cell in order from top to bottom. At the end, you will have a cleaned CSV file ready for SageMaker.\n",
    "\n",
    "**Important:** If you need to start over, use Kernel → Restart & Clear Output before re-running.\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing a Good Dataset\n",
    "\n",
    "### Size Requirements\n",
    "- **Minimum:** 300 rows (500+ recommended)\n",
    "- **Columns:** At least 4 feature columns plus your target\n",
    "\n",
    "### Data Quality\n",
    "- Prefer datasets with mostly numeric columns\n",
    "- Avoid text-heavy datasets (reviews, descriptions)\n",
    "- Look for datasets described as \"clean\"\n",
    "\n",
    "### Thresholds This Notebook Uses\n",
    "\n",
    "| Threshold | Action |\n",
    "|-----------|--------|\n",
    "| >30% missing | Column removed |\n",
    "| >50 text categories | Column removed |\n",
    "| ≤25 text categories | One-hot encoded (dummy variables) |\n",
    "| 26-50 text categories | Frequency encoded |\n",
    "| >20 target categories | Error |\n",
    "| ≤10 unique target values | Treated as classification |\n",
    "\n",
    "### Terminology\n",
    "\n",
    "| Term | Meaning |\n",
    "|------|---------|\n",
    "| Features | Input columns used for prediction |\n",
    "| Target | The column you want to predict |\n",
    "| Regression | Predicting a number |\n",
    "| Classification | Predicting a category |\n",
    "| One-hot encoding | Converting categories to yes/no columns (also called dummy variables) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 1: Configuration\n",
    "\n",
    "**EDIT THE TWO VARIABLES BELOW**, then run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration set:\n",
      "  File: AmesHousing.csv\n",
      "  Target: SalePrice\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CHANGE THESE TWO VALUES\n",
    "# ============================================================\n",
    "\n",
    "FILE_NAME = \"your_dataset.csv\"       # Your CSV file name\n",
    "TARGET_COLUMN = \"your_target\"        # The column you want to predict\n",
    "\n",
    "# ============================================================\n",
    "# DO NOT CHANGE ANYTHING BELOW THIS LINE\n",
    "# ============================================================\n",
    "\n",
    "# Thresholds for automated cleaning\n",
    "MISSING_THRESHOLD = 0.30          # Drop columns with more than 30% missing\n",
    "HIGH_CARDINALITY_THRESHOLD = 50   # Drop text columns with more than 50 unique values\n",
    "ONEHOT_THRESHOLD = 25             # One-hot encode if 25 or fewer unique values\n",
    "MAX_TEXT_CATEGORIES = 20          # Maximum categories allowed for text target columns\n",
    "\n",
    "# Validate thresholds (in case someone edits them)\n",
    "assert 0 < MISSING_THRESHOLD <= 1, \"MISSING_THRESHOLD must be between 0 and 1\"\n",
    "assert HIGH_CARDINALITY_THRESHOLD > 0, \"HIGH_CARDINALITY_THRESHOLD must be positive\"\n",
    "assert ONEHOT_THRESHOLD > 0, \"ONEHOT_THRESHOLD must be positive\"\n",
    "\n",
    "# Strip any accidental whitespace from user input\n",
    "FILE_NAME = FILE_NAME.strip()\n",
    "TARGET_COLUMN = TARGET_COLUMN.strip()\n",
    "\n",
    "print(f\"Configuration set:\")\n",
    "print(f\"  File: {FILE_NAME}\")\n",
    "print(f\"  Target: {TARGET_COLUMN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking variables initialized.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRACKING VARIABLES (initialized for safety)\n",
    "# ============================================================\n",
    "# These track what changes are made during processing.\n",
    "# Do not modify - they are set automatically by later steps.\n",
    "\n",
    "PROBLEM_TYPE = None\n",
    "ORIGINAL_CLASS_MAPPING = None\n",
    "RAW_ROWS = 0\n",
    "RAW_COLS = 0\n",
    "COLS_DROPPED_USELESS = 0\n",
    "COLS_DROPPED_MISSING = 0\n",
    "COLS_DROPPED_CARDINALITY = 0\n",
    "COLS_ENCODED = 0\n",
    "ROWS_DROPPED_MISSING = 0\n",
    "ROWS_DROPPED_DUPLICATES = 0\n",
    "\n",
    "print(\"Tracking variables initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 2: Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded successfully.\n",
      "  pandas version: 2.3.3\n",
      "  numpy version: 1.26.4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Only suppress version-related warnings, keep data warnings visible\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "print(\"Libraries loaded successfully.\")\n",
    "print(f\"  pandas version: {pd.__version__}\")\n",
    "print(f\"  numpy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 3: Load and Inspect Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "NOTE: Potential date/time columns detected:\n",
      "  - Year Built\n",
      "  - Year Remod/Add\n",
      "\n",
      "Date columns are currently processed as text and may be removed.\n",
      "For better results, consider converting dates to separate columns\n",
      "(Year, Month, Day of Week) in Excel before uploading.\n",
      "------------------------------------------------------------\n",
      "\n",
      "============================================================\n",
      "DATA LOADED SUCCESSFULLY\n",
      "============================================================\n",
      "Rows: 2,930\n",
      "Columns: 82\n",
      "Target column: SalePrice\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>PID</th>\n",
       "      <th>MS SubClass</th>\n",
       "      <th>MS Zoning</th>\n",
       "      <th>Lot Frontage</th>\n",
       "      <th>Lot Area</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>Lot Shape</th>\n",
       "      <th>Land Contour</th>\n",
       "      <th>...</th>\n",
       "      <th>Pool Area</th>\n",
       "      <th>Pool QC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>Misc Feature</th>\n",
       "      <th>Misc Val</th>\n",
       "      <th>Mo Sold</th>\n",
       "      <th>Yr Sold</th>\n",
       "      <th>Sale Type</th>\n",
       "      <th>Sale Condition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>526301100</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>141.0</td>\n",
       "      <td>31770</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>215000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>526350040</td>\n",
       "      <td>20</td>\n",
       "      <td>RH</td>\n",
       "      <td>80.0</td>\n",
       "      <td>11622</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>105000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>526351010</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>81.0</td>\n",
       "      <td>14267</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Gar2</td>\n",
       "      <td>12500</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>172000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>526353030</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>93.0</td>\n",
       "      <td>11160</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>527105010</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>74.0</td>\n",
       "      <td>13830</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MnPrv</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>189900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Order        PID  MS SubClass MS Zoning  Lot Frontage  Lot Area Street  \\\n",
       "0      1  526301100           20        RL         141.0     31770   Pave   \n",
       "1      2  526350040           20        RH          80.0     11622   Pave   \n",
       "2      3  526351010           20        RL          81.0     14267   Pave   \n",
       "3      4  526353030           20        RL          93.0     11160   Pave   \n",
       "4      5  527105010           60        RL          74.0     13830   Pave   \n",
       "\n",
       "  Alley Lot Shape Land Contour  ... Pool Area Pool QC  Fence Misc Feature  \\\n",
       "0   NaN       IR1          Lvl  ...         0     NaN    NaN          NaN   \n",
       "1   NaN       Reg          Lvl  ...         0     NaN  MnPrv          NaN   \n",
       "2   NaN       IR1          Lvl  ...         0     NaN    NaN         Gar2   \n",
       "3   NaN       Reg          Lvl  ...         0     NaN    NaN          NaN   \n",
       "4   NaN       IR1          Lvl  ...         0     NaN  MnPrv          NaN   \n",
       "\n",
       "  Misc Val Mo Sold Yr Sold Sale Type  Sale Condition  SalePrice  \n",
       "0        0       5    2010       WD           Normal     215000  \n",
       "1        0       6    2010       WD           Normal     105000  \n",
       "2    12500       6    2010       WD           Normal     172000  \n",
       "3        0       4    2010       WD           Normal     244000  \n",
       "4        0       3    2010       WD           Normal     189900  \n",
       "\n",
       "[5 rows x 82 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Helper function to detect delimiter (ENHANCED: examines multiple lines for robustness)\n",
    "def detect_delimiter(filename):\n",
    "    \"\"\"Auto-detect CSV delimiter by examining multiple lines for consistency.\"\"\"\n",
    "    # Read first 5 lines for more robust detection\n",
    "    lines = []\n",
    "    used_encoding = 'utf-8-sig'\n",
    "    for encoding in ['utf-8-sig', 'utf-8', 'latin1']:\n",
    "        try:\n",
    "            with open(filename, 'r', encoding=encoding) as f:\n",
    "                lines = [f.readline() for _ in range(5)]\n",
    "            used_encoding = encoding\n",
    "            break\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    \n",
    "    if not lines or not any(line.strip() for line in lines):\n",
    "        return ',', 'comma'\n",
    "    \n",
    "    # Score each delimiter by consistency across lines\n",
    "    best_delimiter = ','\n",
    "    best_score = 0\n",
    "    \n",
    "    for delimiter in ['\\t', ';', ',']:\n",
    "        try:\n",
    "            counts = []\n",
    "            for line in lines:\n",
    "                if line.strip():\n",
    "                    reader = csv.reader([line], delimiter=delimiter)\n",
    "                    counts.append(len(next(reader)))\n",
    "            \n",
    "            if counts and len(set(counts)) == 1 and counts[0] > 1:\n",
    "                # Consistent field count across lines - good sign\n",
    "                if counts[0] > best_score:\n",
    "                    best_score = counts[0]\n",
    "                    best_delimiter = delimiter\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    delim_names = {'\\t': 'tab', ';': 'semicolon', ',': 'comma'}\n",
    "    return best_delimiter, delim_names.get(best_delimiter, 'comma')\n",
    "\n",
    "# Detect delimiter first\n",
    "try:\n",
    "    delimiter, delim_name = detect_delimiter(FILE_NAME)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Could not find file '{FILE_NAME}'\")\n",
    "    print(f\"Make sure the file is in the same folder as this notebook.\")\n",
    "    raise\n",
    "\n",
    "# Load the data (FIX: use utf-8-sig to handle BOM)\n",
    "try:\n",
    "    df = pd.read_csv(FILE_NAME, sep=delimiter, encoding='utf-8-sig')\n",
    "    if delim_name != 'comma':\n",
    "        print(f\"Detected {delim_name}-separated file.\")\n",
    "except UnicodeDecodeError:\n",
    "    # Try alternative encodings\n",
    "    for encoding in ['latin1', 'iso-8859-1', 'cp1252']:\n",
    "        try:\n",
    "            df = pd.read_csv(FILE_NAME, sep=delimiter, encoding=encoding)\n",
    "            print(f\"Loaded with {encoding} encoding.\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"ERROR: Could not read file. Try saving it as UTF-8.\")\n",
    "        raise\n",
    "\n",
    "# Strip whitespace from column names (common source of errors)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Check if file is empty\n",
    "if len(df) == 0:\n",
    "    print(f\"ERROR: The file '{FILE_NAME}' contains no data rows.\")\n",
    "    print(\"Please ensure your CSV file contains data, not just headers.\")\n",
    "    raise ValueError(\"CSV file is empty (no data rows).\")\n",
    "\n",
    "# Check if target column exists (with auto-correction for case mismatches)\n",
    "if TARGET_COLUMN not in df.columns:\n",
    "    # Check for close matches (case differences, extra spaces)\n",
    "    close_matches = [c for c in df.columns if c.lower().strip() == TARGET_COLUMN.lower().strip()]\n",
    "    if len(close_matches) == 1:\n",
    "        # Auto-correct if exactly one case-insensitive match\n",
    "        print(f\"Note: Using '{close_matches[0]}' (case-corrected from '{TARGET_COLUMN}')\")\n",
    "        TARGET_COLUMN = close_matches[0]\n",
    "    else:\n",
    "        print(f\"ERROR: Could not find column '{TARGET_COLUMN}'\")\n",
    "        print(f\"\")\n",
    "        if close_matches:\n",
    "            print(f\"Did you mean one of these? {close_matches}\")\n",
    "            print(f\"\")\n",
    "        print(f\"Available columns are:\")\n",
    "        for col in df.columns:\n",
    "            print(f\"  - {col}\")\n",
    "        raise ValueError(f\"Target column '{TARGET_COLUMN}' not found.\")\n",
    "\n",
    "# Save initial dimensions for final report\n",
    "RAW_ROWS = len(df)\n",
    "RAW_COLS = len(df.columns)\n",
    "\n",
    "# Warn about very small datasets\n",
    "if RAW_ROWS < 300:\n",
    "    print(f\"WARNING: The file contains only {RAW_ROWS} rows.\")\n",
    "    print(\"Machine learning typically requires hundreds or thousands of rows.\")\n",
    "    print(\"\")\n",
    "\n",
    "# Check for potential date/time columns\n",
    "date_patterns = ['date', 'time', 'year', 'month', 'day', 'week', \n",
    "                 'timestamp', 'created', 'updated', 'modified', 'period']\n",
    "potential_date_cols = [col for col in df.columns \n",
    "                       if any(pattern in col.lower() for pattern in date_patterns)]\n",
    "\n",
    "# Also check for columns that look like dates based on content\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    if col not in potential_date_cols and col != TARGET_COLUMN:\n",
    "        # Sample a few values to check for date-like patterns\n",
    "        sample = df[col].dropna().head(5).astype(str)\n",
    "        # Common date patterns: YYYY-MM-DD, MM/DD/YYYY, etc.\n",
    "        date_like = sample.str.match(r'^\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}').any()\n",
    "        if date_like:\n",
    "            potential_date_cols.append(col)\n",
    "\n",
    "if potential_date_cols:\n",
    "    print(\"-\" * 60)\n",
    "    print(\"NOTE: Potential date/time columns detected:\")\n",
    "    for col in potential_date_cols:\n",
    "        print(f\"  - {col}\")\n",
    "    print()\n",
    "    print(\"Date columns are currently processed as text and may be removed.\")\n",
    "    print(\"For better results, consider converting dates to separate columns\")\n",
    "    print(\"(Year, Month, Day of Week) in Excel before uploading.\")\n",
    "    print(\"-\" * 60)\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA LOADED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Rows: {RAW_ROWS:,}\")\n",
    "print(f\"Columns: {RAW_COLS}\")\n",
    "print(f\"Target column: {TARGET_COLUMN}\")\n",
    "print(f\"\")\n",
    "print(\"First few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Overview\n",
    "\n",
    "Review the columns loaded above. Consider:\n",
    "- What type of information does this dataset contain?\n",
    "- What patterns might exist in this data?\n",
    "- Is the target column appropriate for your prediction goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 4: Detect Problem Type\n",
    "\n",
    "The notebook determines if this is regression or classification.\n",
    "\n",
    "### Problem Types\n",
    "\n",
    "**Regression** - predicting a number:\n",
    "- Price, revenue, count, percentage\n",
    "\n",
    "**Classification** - predicting a category:\n",
    "- Yes/No, Low/Medium/High, Category A/B/C\n",
    "\n",
    "### Detection Rules\n",
    "\n",
    "1. Text target with ≤20 categories → Classification\n",
    "2. Numeric target with ≤10 unique values → Classification  \n",
    "3. Numeric target with >10 unique values → Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANALYZING TARGET COLUMN\n",
      "============================================================\n",
      "Column: SalePrice\n",
      "Unique values: 1032\n",
      "Data type: int64\n",
      "\n",
      "Detected: REGRESSION (predicting a number)\n",
      "\n",
      "------------------------------------------------------------\n",
      "Does this match your prediction from above?\n",
      "If not, review the 'After Running the Next Cell' section.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Analyze the target column to determine problem type\n",
    "target_values = df[TARGET_COLUMN].dropna()\n",
    "n_unique = target_values.nunique()\n",
    "target_dtype = target_values.dtype\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ANALYZING TARGET COLUMN\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Column: {TARGET_COLUMN}\")\n",
    "print(f\"Unique values: {n_unique}\")\n",
    "print(f\"Data type: {target_dtype}\")\n",
    "print(\"\")\n",
    "\n",
    "# Check for degenerate case first (must have at least 2 unique values)\n",
    "if n_unique < 2:\n",
    "    print(f\"ERROR: Target column '{TARGET_COLUMN}' has only {n_unique} unique value(s).\")\n",
    "    print(\"A model cannot learn to predict when there is no variation in the target.\")\n",
    "    print(\"Please check your dataset or choose a different target column.\")\n",
    "    raise ValueError(f\"Target column must have at least 2 unique values.\")\n",
    "\n",
    "# Determine problem type (ENHANCED: better handling of float vs int targets)\n",
    "if target_dtype == 'object':\n",
    "    # Text target - must be classification\n",
    "    if n_unique > MAX_TEXT_CATEGORIES:\n",
    "        print(f\"ERROR: Target column has {n_unique} text categories.\")\n",
    "        print(f\"This is too many for classification (maximum {MAX_TEXT_CATEGORIES}).\")\n",
    "        print(\"Consider:\")\n",
    "        print(\"  - Grouping categories into fewer buckets\")\n",
    "        print(\"  - Using a different target column\")\n",
    "        print(\"  - Using a different dataset\")\n",
    "        raise ValueError(f\"Too many categories ({n_unique}) in text target column.\")\n",
    "    elif n_unique == 2:\n",
    "        PROBLEM_TYPE = 'binary_classification'\n",
    "        print(\"Detected: BINARY CLASSIFICATION (two text categories)\")\n",
    "    else:\n",
    "        PROBLEM_TYPE = 'multiclass_classification'\n",
    "        print(f\"Detected: MULTICLASS CLASSIFICATION ({n_unique} text categories)\")\n",
    "elif pd.api.types.is_integer_dtype(target_values) and n_unique <= 10:\n",
    "    # Integer with few unique values - treat as classification\n",
    "    if n_unique == 2:\n",
    "        PROBLEM_TYPE = 'binary_classification'\n",
    "        print(\"Detected: BINARY CLASSIFICATION (two numeric categories)\")\n",
    "    else:\n",
    "        PROBLEM_TYPE = 'multiclass_classification'\n",
    "        print(f\"Detected: MULTICLASS CLASSIFICATION ({n_unique} numeric categories)\")\n",
    "elif pd.api.types.is_float_dtype(target_values) and n_unique == 2:\n",
    "    # Float with only 2 unique values - check if 0.0 and 1.0\n",
    "    unique_vals = sorted(target_values.unique())\n",
    "    if unique_vals == [0.0, 1.0]:\n",
    "        PROBLEM_TYPE = 'binary_classification'\n",
    "        print(\"Detected: BINARY CLASSIFICATION (0.0 and 1.0 values)\")\n",
    "    else:\n",
    "        # ENHANCED: Provide guidance for ambiguous 2-value float targets\n",
    "        PROBLEM_TYPE = 'regression'\n",
    "        print(\"Detected: REGRESSION (predicting a number)\")\n",
    "        print(f\"\")\n",
    "        print(f\"Note: Your target has only 2 unique float values: {unique_vals}\")\n",
    "        print(\"If this should be classification (predicting categories), convert your\")\n",
    "        print(\"target column to integers or text first, then re-run the notebook.\")\n",
    "elif pd.api.types.is_float_dtype(target_values) and n_unique <= 10:\n",
    "    # ENHANCED: Provide note for float targets with few unique values\n",
    "    PROBLEM_TYPE = 'regression'\n",
    "    print(\"Detected: REGRESSION (predicting a number)\")\n",
    "    print(f\"\")\n",
    "    print(f\"Note: Your target has only {n_unique} unique values.\")\n",
    "    print(\"If these represent categories (like ratings 1-5), consider converting\")\n",
    "    print(\"the target to integers for classification instead of regression.\")\n",
    "else:\n",
    "    PROBLEM_TYPE = 'regression'\n",
    "    print(\"Detected: REGRESSION (predicting a number)\")\n",
    "\n",
    "# For classification, encode target as integers\n",
    "# (FIX: store original mapping for potential re-encoding later)\n",
    "ORIGINAL_CLASS_MAPPING = None\n",
    "if PROBLEM_TYPE in ['binary_classification', 'multiclass_classification']:\n",
    "    # First report any missing targets\n",
    "    missing_target_count = df[TARGET_COLUMN].isna().sum()\n",
    "    if missing_target_count > 0:\n",
    "        print(f\"\")\n",
    "        print(f\"Note: {missing_target_count} rows have missing target values (will be removed later).\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Converting categories to numbers:\")\n",
    "    \n",
    "    # FIX: Handle mixed types by converting to string first\n",
    "    try:\n",
    "        unique_classes = sorted(df[TARGET_COLUMN].dropna().unique())\n",
    "    except TypeError:\n",
    "        # Mixed types - convert all to string first\n",
    "        print(\"Note: Target column has mixed types. Converting all to text.\")\n",
    "        df[TARGET_COLUMN] = df[TARGET_COLUMN].astype(str)\n",
    "        unique_classes = sorted(df[TARGET_COLUMN].dropna().unique())\n",
    "    \n",
    "    ORIGINAL_CLASS_MAPPING = {val: idx for idx, val in enumerate(unique_classes)}\n",
    "    df[TARGET_COLUMN] = df[TARGET_COLUMN].map(ORIGINAL_CLASS_MAPPING)\n",
    "    for original, encoded in ORIGINAL_CLASS_MAPPING.items():\n",
    "        print(f\"  {original} -> {encoded}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"-\" * 60)\n",
    "print(\"Does this match your prediction from above?\")\n",
    "print(\"If not, review the 'After Running the Next Cell' section.\")\n",
    "print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 5: Remove Useless Columns\n",
    "\n",
    "This step removes columns that would confuse the model:\n",
    "- ID columns (just labels, not useful for prediction)\n",
    "- Constant columns (same value in every row, provides no information)\n",
    "\n",
    "### Why These Columns Get Removed\n",
    "\n",
    "**ID columns** (like customer_id, order_number): These are just labels that identify each row. They do not contain information that helps predict outcomes. Think of them like Social Security Numbers - unique to each person but not predictive of behavior.\n",
    "\n",
    "**Constant columns**: If every row has the same value, that column cannot help distinguish between different outcomes. A column where everyone has the same value provides zero predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "REMOVING USELESS COLUMNS\n",
      "============================================================\n",
      "Removed 1 column(s):\n",
      "  - 'Order' - sequential ID column\n",
      "\n",
      "Data shape: 2,930 rows x 81 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"REMOVING USELESS COLUMNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cols_to_drop = []\n",
    "drop_reasons = []\n",
    "\n",
    "# ID-like column name patterns (whole word matches only)\n",
    "ID_EXACT_NAMES = {'id', 'index', 'rownum', 'row_num', 'record', 'record_id', 'row_id', \n",
    "                  'idx', 'pk', 'key', 'guid', 'uuid'}\n",
    "ID_SUFFIXES = {'_id', '_index', '_idx', '_pk', '_key'}\n",
    "ID_PREFIXES = {'id_', 'index_', 'idx_'}\n",
    "\n",
    "for col in df.columns:\n",
    "    if col == TARGET_COLUMN:\n",
    "        continue\n",
    "    \n",
    "    col_lower = col.lower()\n",
    "    \n",
    "    # Check for ID-like column names (whole word match, not substring)\n",
    "    is_id_name = (\n",
    "        col_lower in ID_EXACT_NAMES or\n",
    "        any(col_lower.endswith(suffix) for suffix in ID_SUFFIXES) or\n",
    "        any(col_lower.startswith(prefix) for prefix in ID_PREFIXES)\n",
    "    )\n",
    "    \n",
    "    # Check if values are unique (strong ID indicator)\n",
    "    is_unique = df[col].nunique() == len(df)\n",
    "    \n",
    "    # Check if values are sequential integers (very strong ID indicator)\n",
    "    is_sequential = False\n",
    "    if pd.api.types.is_numeric_dtype(df[col]):\n",
    "        col_sorted = df[col].dropna().sort_values().reset_index(drop=True)\n",
    "        if len(col_sorted) > 1:\n",
    "            diffs = col_sorted.diff().dropna()\n",
    "            if len(diffs) > 0 and ((diffs == 1).all() or (diffs == -1).all()):\n",
    "                is_sequential = True\n",
    "    \n",
    "    # ENHANCED: Check if column looks like categorical data despite unique values\n",
    "    # This prevents removing columns like 'category_id' that contain meaningful categories\n",
    "    is_likely_categorical = False\n",
    "    if df[col].dtype == 'object' and is_unique:\n",
    "        # If values are short strings (< 30 chars avg), they might be categories not IDs\n",
    "        avg_len = df[col].astype(str).str.len().mean()\n",
    "        # Also check if values look like UUIDs or long codes\n",
    "        sample_vals = df[col].dropna().head(5).astype(str)\n",
    "        looks_like_uuid = any(len(str(v)) > 30 or '-' in str(v) for v in sample_vals)\n",
    "        if avg_len < 25 and not looks_like_uuid:\n",
    "            is_likely_categorical = True\n",
    "    \n",
    "    # Only flag as ID if: (name looks like ID AND unique AND not categorical) OR (sequential integers)\n",
    "    if (is_id_name and is_unique and not is_likely_categorical) or (is_sequential and is_unique):\n",
    "        cols_to_drop.append(col)\n",
    "        if is_sequential:\n",
    "            drop_reasons.append(f\"'{col}' - sequential ID column\")\n",
    "        else:\n",
    "            drop_reasons.append(f\"'{col}' - ID column (just a label)\")\n",
    "        continue\n",
    "    \n",
    "    # Check for constant columns (all same value)\n",
    "    if df[col].nunique() <= 1:  # <= 1 handles all-NaN case too\n",
    "        cols_to_drop.append(col)\n",
    "        drop_reasons.append(f\"'{col}' - constant (all values identical)\")\n",
    "\n",
    "# Execute drops\n",
    "if cols_to_drop:\n",
    "    df = df.drop(columns=cols_to_drop)\n",
    "    print(f\"Removed {len(cols_to_drop)} column(s):\")\n",
    "    for reason in drop_reasons:\n",
    "        print(f\"  - {reason}\")\n",
    "else:\n",
    "    print(\"No useless columns found.\")\n",
    "\n",
    "COLS_DROPPED_USELESS = len(cols_to_drop)\n",
    "\n",
    "# Check if any features remain\n",
    "remaining_features = [c for c in df.columns if c != TARGET_COLUMN]\n",
    "if len(remaining_features) == 0:\n",
    "    print(\"\")\n",
    "    print(\"!\" * 60)\n",
    "    print(\"ERROR: NO FEATURES REMAINING\")\n",
    "    print(\"!\" * 60)\n",
    "    print(\"All columns except the target were removed.\")\n",
    "    print(\"This may indicate:\")\n",
    "    print(\"  - All columns look like ID columns\")\n",
    "    print(\"  - All columns have identical values\")\n",
    "    print(\"Please choose a different dataset.\")\n",
    "    raise ValueError(\"No feature columns remaining.\")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"Data shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 6: Handle Missing Values\n",
    "\n",
    "Missing values cause problems for machine learning. This step:\n",
    "- Removes rows where the target is missing (we cannot learn from these)\n",
    "- Removes columns with too many missing values\n",
    "- Fills remaining gaps with reasonable estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HANDLING MISSING VALUES\n",
      "============================================================\n",
      "Removed 6 column(s) with more than 30% missing:\n",
      "  - Alley\n",
      "  - Mas Vnr Type\n",
      "  - Fireplace Qu\n",
      "  - Pool QC\n",
      "  - Fence\n",
      "  - Misc Feature\n",
      "\n",
      "Rows: 2,930 -> 2,930 (0 removed)\n",
      "Columns: 81 -> 75 (6 removed)\n",
      "\n",
      "Data shape: 2,930 rows x 75 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rows_before = len(df)\n",
    "cols_before = len(df.columns)\n",
    "\n",
    "# 1. Drop rows with missing target\n",
    "missing_target = df[TARGET_COLUMN].isna().sum()\n",
    "if missing_target > 0:\n",
    "    df = df.dropna(subset=[TARGET_COLUMN])\n",
    "    print(f\"Removed {missing_target} rows with missing target values.\")\n",
    "\n",
    "# Check if we have any data left after removing missing targets\n",
    "if len(df) == 0:\n",
    "    print(\"\")\n",
    "    print(\"!\" * 60)\n",
    "    print(\"ERROR: ALL ROWS REMOVED\")\n",
    "    print(\"!\" * 60)\n",
    "    print(\"Every row had a missing target value.\")\n",
    "    print(\"Please check your target column or choose a different dataset.\")\n",
    "    raise ValueError(\"No rows remaining after removing missing targets.\")\n",
    "\n",
    "# 2. Drop columns with too many missing values\n",
    "cols_dropped_missing = []\n",
    "for col in df.columns:\n",
    "    if col == TARGET_COLUMN:\n",
    "        continue\n",
    "    missing_pct = df[col].isna().sum() / len(df)\n",
    "    if missing_pct > MISSING_THRESHOLD:\n",
    "        cols_dropped_missing.append(col)\n",
    "\n",
    "if cols_dropped_missing:\n",
    "    df = df.drop(columns=cols_dropped_missing)\n",
    "    print(f\"Removed {len(cols_dropped_missing)} column(s) with more than {MISSING_THRESHOLD*100:.0f}% missing:\")\n",
    "    for col in cols_dropped_missing:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# FIX: Convert boolean columns to integer before processing\n",
    "bool_cols = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "if bool_cols:\n",
    "    for col in bool_cols:\n",
    "        df[col] = df[col].astype(int)\n",
    "    print(f\"Converted {len(bool_cols)} boolean column(s) to integers.\")\n",
    "\n",
    "# ENHANCED: Replace infinite values with NaN before median calculation\n",
    "# This prevents median from being inf and ensures proper handling\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "inf_count = 0\n",
    "for col in numeric_cols:\n",
    "    col_inf = np.isinf(df[col]).sum()\n",
    "    if col_inf > 0:\n",
    "        inf_count += col_inf\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "if inf_count > 0:\n",
    "    print(f\"Replaced {inf_count} infinite value(s) with missing (will be filled with median).\")\n",
    "\n",
    "# 3. Fill remaining missing values\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "# Fill numeric with median (handle all-NaN columns)\n",
    "cols_dropped_all_nan = []\n",
    "for col in numeric_cols:\n",
    "    if col != TARGET_COLUMN and df[col].isna().sum() > 0:\n",
    "        median_val = df[col].median()\n",
    "        if pd.isna(median_val):\n",
    "            # Column is entirely NaN - drop it\n",
    "            cols_dropped_all_nan.append(col)\n",
    "        else:\n",
    "            df.loc[:, col] = df[col].fillna(median_val)\n",
    "\n",
    "if cols_dropped_all_nan:\n",
    "    df = df.drop(columns=cols_dropped_all_nan)\n",
    "    print(f\"Removed {len(cols_dropped_all_nan)} column(s) with all missing values:\")\n",
    "    for col in cols_dropped_all_nan:\n",
    "        print(f\"  - {col}\")\n",
    "\n",
    "# Fill categorical with 'Missing'\n",
    "for col in categorical_cols:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        df.loc[:, col] = df[col].fillna('Missing')\n",
    "\n",
    "rows_after = len(df)\n",
    "cols_after = len(df.columns)\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Rows: {rows_before:,} -> {rows_after:,} ({rows_before - rows_after:,} removed)\")\n",
    "print(f\"Columns: {cols_before} -> {cols_after} ({cols_before - cols_after} removed)\")\n",
    "\n",
    "# Check if we still have data\n",
    "if len(df) < 10:\n",
    "    print(\"\")\n",
    "    print(\"!\" * 60)\n",
    "    print(\"ERROR: TOO FEW ROWS REMAINING\")\n",
    "    print(\"!\" * 60)\n",
    "    print(f\"Only {len(df)} rows remain after cleaning.\")\n",
    "    print(\"This dataset has too many missing values to use.\")\n",
    "    print(\"Please choose a different dataset.\")\n",
    "    raise ValueError(\"Insufficient data remaining.\")\n",
    "\n",
    "# Track for final summary\n",
    "ROWS_DROPPED_MISSING = rows_before - rows_after\n",
    "COLS_DROPPED_MISSING = len(cols_dropped_missing) + len(cols_dropped_all_nan)\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"Data shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 7: Encode Categorical Variables\n",
    "\n",
    "Machine learning requires all data to be numeric. This step converts text columns to numbers:\n",
    "- Columns with few categories become multiple yes/no columns (one-hot encoding, also known as **dummy variables**)\n",
    "- Columns with many categories are converted based on frequency\n",
    "- Columns with too many categories are removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENCODING NON-NUMERIC COLUMNS\n",
      "============================================================\n",
      "  'MS Zoning' (7 categories) -> 6 dummy column(s)\n",
      "  'Street' (2 categories) -> 1 dummy column(s)\n",
      "  'Lot Shape' (4 categories) -> 3 dummy column(s)\n",
      "  'Land Contour' (4 categories) -> 3 dummy column(s)\n",
      "  'Utilities' (3 categories) -> 2 dummy column(s)\n",
      "  'Lot Config' (5 categories) -> 4 dummy column(s)\n",
      "  'Land Slope' (3 categories) -> 2 dummy column(s)\n",
      "  'Neighborhood' (28 categories) -> frequency values\n",
      "  'Condition 1' (9 categories) -> 8 dummy column(s)\n",
      "  'Condition 2' (8 categories) -> 7 dummy column(s)\n",
      "  'Bldg Type' (5 categories) -> 4 dummy column(s)\n",
      "  'House Style' (8 categories) -> 7 dummy column(s)\n",
      "  'Roof Style' (6 categories) -> 5 dummy column(s)\n",
      "  'Roof Matl' (8 categories) -> 7 dummy column(s)\n",
      "  'Exterior 1st' (16 categories) -> 15 dummy column(s)\n",
      "  'Exterior 2nd' (17 categories) -> 16 dummy column(s)\n",
      "  'Exter Qual' (4 categories) -> 3 dummy column(s)\n",
      "  'Exter Cond' (5 categories) -> 4 dummy column(s)\n",
      "  'Foundation' (6 categories) -> 5 dummy column(s)\n",
      "  'Bsmt Qual' (6 categories) -> 5 dummy column(s)\n",
      "  'Bsmt Cond' (6 categories) -> 5 dummy column(s)\n",
      "  'Bsmt Exposure' (5 categories) -> 4 dummy column(s)\n",
      "  'BsmtFin Type 1' (7 categories) -> 6 dummy column(s)\n",
      "  'BsmtFin Type 2' (7 categories) -> 6 dummy column(s)\n",
      "  'Heating' (6 categories) -> 5 dummy column(s)\n",
      "  'Heating QC' (5 categories) -> 4 dummy column(s)\n",
      "  'Central Air' (2 categories) -> 1 dummy column(s)\n",
      "  'Electrical' (6 categories) -> 5 dummy column(s)\n",
      "  'Kitchen Qual' (5 categories) -> 4 dummy column(s)\n",
      "  'Functional' (8 categories) -> 7 dummy column(s)\n",
      "  'Garage Type' (7 categories) -> 6 dummy column(s)\n",
      "  'Garage Finish' (4 categories) -> 3 dummy column(s)\n",
      "  'Garage Qual' (6 categories) -> 5 dummy column(s)\n",
      "  'Garage Cond' (6 categories) -> 5 dummy column(s)\n",
      "  'Paved Drive' (3 categories) -> 2 dummy column(s)\n",
      "  'Sale Type' (10 categories) -> 9 dummy column(s)\n",
      "  'Sale Condition' (6 categories) -> 5 dummy column(s)\n",
      "\n",
      "Encoded 37 column(s).\n",
      "\n",
      "Data shape: 2,930 rows x 228 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ENCODING NON-NUMERIC COLUMNS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# FIRST: Attempt to convert \"dirty\" numeric columns (currency, commas, percentages)\n",
    "# Business data often has values like \"$1,200.00\" or \"1,000\" stored as text\n",
    "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "cols_converted_to_numeric = []\n",
    "\n",
    "for col in object_cols:\n",
    "    if col == TARGET_COLUMN:\n",
    "        continue\n",
    "    \n",
    "    # Try to clean and convert to numeric\n",
    "    # Remove common formatting: $, commas, %, leading/trailing spaces\n",
    "    cleaned = df[col].astype(str).str.strip()\n",
    "    cleaned = cleaned.str.replace(r'[$,]', '', regex=True)\n",
    "    cleaned = cleaned.str.replace(r'%$', '', regex=True)\n",
    "    \n",
    "    # Attempt conversion\n",
    "    numeric_version = pd.to_numeric(cleaned, errors='coerce')\n",
    "    \n",
    "    # Only convert if 90%+ of non-null values successfully converted\n",
    "    non_null_count = df[col].notna().sum()\n",
    "    if non_null_count > 0:\n",
    "        success_rate = numeric_version.notna().sum() / non_null_count\n",
    "        if success_rate >= 0.90:\n",
    "            df[col] = numeric_version\n",
    "            cols_converted_to_numeric.append(col)\n",
    "\n",
    "if cols_converted_to_numeric:\n",
    "    print(f\"Converted {len(cols_converted_to_numeric)} text column(s) to numbers:\")\n",
    "    for col in cols_converted_to_numeric:\n",
    "        print(f\"  - {col} (detected currency/number formatting)\")\n",
    "    print()\n",
    "\n",
    "# NOW proceed with categorical encoding for remaining object columns\n",
    "categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "if not categorical_cols:\n",
    "    print(\"No non-numeric columns to encode.\")\n",
    "    COLS_ENCODED = 0\n",
    "    COLS_DROPPED_CARDINALITY = 0\n",
    "else:\n",
    "    cols_encoded = 0\n",
    "    cols_dropped_cardinality = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        n_unique = df[col].nunique()\n",
    "        \n",
    "        if n_unique <= ONEHOT_THRESHOLD:\n",
    "            # One-hot encode (creates separate columns for each category)\n",
    "            # drop_first=True removes one category to avoid multicollinearity\n",
    "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True, dtype=int)\n",
    "            \n",
    "            # Check for naming conflicts with existing columns\n",
    "            conflicting_cols = set(dummies.columns) & set(df.columns) - {col}\n",
    "            if conflicting_cols:\n",
    "                # Rename conflicting columns to avoid silent overwrites\n",
    "                dummies = dummies.rename(columns={c: f\"{c}_enc\" for c in conflicting_cols})\n",
    "                print(f\"  Note: Renamed {len(conflicting_cols)} column(s) to avoid naming conflicts.\")\n",
    "            \n",
    "            df = pd.concat([df.drop(columns=[col]), dummies], axis=1)\n",
    "            print(f\"  '{col}' ({n_unique} categories) -> {len(dummies.columns)} dummy column(s)\")\n",
    "            cols_encoded += 1\n",
    "            \n",
    "        elif n_unique <= HIGH_CARDINALITY_THRESHOLD:\n",
    "            # Frequency encode (replace with how common each value is)\n",
    "            # Note: Frequency encoding replaces each category with its occurrence rate.\n",
    "            # This works reasonably for Linear Learner but is not ideal - the model\n",
    "            # assumes a linear relationship between frequency and the target, which\n",
    "            # may not exist. For better results, consider reducing categories or\n",
    "            # using a dataset with fewer high-cardinality text columns.\n",
    "            freq_map = df[col].value_counts(normalize=True).to_dict()\n",
    "            df[col] = df[col].map(freq_map)\n",
    "            \n",
    "            # FIX: Verify no NaN was introduced\n",
    "            if df[col].isna().any():\n",
    "                print(f\"  Warning: Some values in '{col}' could not be frequency encoded. Filling with 0.\")\n",
    "                df.loc[:, col] = df[col].fillna(0)\n",
    "            \n",
    "            print(f\"  '{col}' ({n_unique} categories) -> frequency values\")\n",
    "            cols_encoded += 1\n",
    "            \n",
    "        else:\n",
    "            # Too many unique values - drop the column\n",
    "            df = df.drop(columns=[col])\n",
    "            cols_dropped_cardinality.append(col)\n",
    "            print(f\"  '{col}' ({n_unique} categories) -> REMOVED (too many categories)\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(f\"Encoded {cols_encoded} column(s).\")\n",
    "    if cols_dropped_cardinality:\n",
    "        print(f\"Removed {len(cols_dropped_cardinality)} column(s) with too many categories.\")\n",
    "    \n",
    "    COLS_ENCODED = cols_encoded\n",
    "    COLS_DROPPED_CARDINALITY = len(cols_dropped_cardinality)\n",
    "\n",
    "# FIX: Check if any features remain after encoding\n",
    "remaining_features = [c for c in df.columns if c != TARGET_COLUMN]\n",
    "if len(remaining_features) == 0:\n",
    "    print(\"\")\n",
    "    print(\"!\" * 60)\n",
    "    print(\"ERROR: NO FEATURES REMAINING\")\n",
    "    print(\"!\" * 60)\n",
    "    print(\"All feature columns were removed due to too many categories.\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"  - Using a dataset with fewer text columns\")\n",
    "    print(\"  - Using a dataset with more rows\")\n",
    "    print(\"  - Increasing HIGH_CARDINALITY_THRESHOLD (advanced)\")\n",
    "    raise ValueError(\"No feature columns remaining after encoding.\")\n",
    "\n",
    "print(f\"\")\n",
    "print(f\"Data shape: {df.shape[0]:,} rows x {df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 8: Correlation Analysis (Optional)\n",
    "\n",
    "This shows which features are most related to your target. Higher values (positive or negative) mean stronger relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CORRELATION ANALYSIS\n",
      "============================================================\n",
      "Top 10 features most related to your target:\n",
      "\n",
      "  Overall Qual                             0.799 ***************\n",
      "  Gr Liv Area                              0.707 **************\n",
      "  Garage Cars                              0.648 ************\n",
      "  Garage Area                              0.640 ************\n",
      "  Total Bsmt SF                            0.632 ************\n",
      "  1st Flr SF                               0.622 ************\n",
      "  Exter Qual_TA                            0.591 ***********\n",
      "  Year Built                               0.558 ***********\n",
      "  Full Bath                                0.546 **********\n",
      "  Year Remod/Add                           0.533 **********\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    if TARGET_COLUMN not in df.columns:\n",
    "        print(\"Warning: Target column not found. Skipping correlation analysis.\")\n",
    "    elif PROBLEM_TYPE == 'regression' or PROBLEM_TYPE == 'binary_classification':\n",
    "        # Calculate correlations\n",
    "        corr_matrix = df.corr(numeric_only=True)\n",
    "        \n",
    "        # FIX: Check if target is in correlation matrix\n",
    "        if TARGET_COLUMN not in corr_matrix.columns:\n",
    "            print(\"Warning: Could not compute correlations (target column has no variance).\")\n",
    "        else:\n",
    "            correlations = corr_matrix[TARGET_COLUMN].drop(TARGET_COLUMN, errors='ignore')\n",
    "            correlations = correlations.abs().sort_values(ascending=False)\n",
    "            \n",
    "            print(\"Top 10 features most related to your target:\")\n",
    "            print(\"\")\n",
    "            for feature, corr in correlations.head(10).items():\n",
    "                bar = \"*\" * int(corr * 20)\n",
    "                # FIX: Better handling of long feature names\n",
    "                display_name = feature if len(feature) <= 40 else feature[:37] + \"...\"\n",
    "                print(f\"  {display_name:<40} {corr:.3f} {bar}\")\n",
    "    else:\n",
    "        print(\"Correlation analysis is most useful for regression and binary classification.\")\n",
    "        print(\"Skipping for multiclass classification.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not compute correlations ({e}).\")\n",
    "    print(\"Continuing with data preparation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 9: Final Validation and Formatting\n",
    "\n",
    "This step prepares the data in the exact format required by AWS SageMaker Linear Learner:\n",
    "- Target column first\n",
    "- All values converted to decimal numbers\n",
    "- No missing values\n",
    "- Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL VALIDATION\n",
      "============================================================\n",
      "All columns are numeric.\n",
      "No infinite values.\n",
      "Missing values handled.\n",
      "No missing values.\n",
      "No duplicate rows.\n",
      "Converted all values to float32.\n",
      "Moved target column to first position.\n",
      "\n",
      "Final dataset: 2,930 rows x 228 columns\n",
      "Features: 227\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FINAL VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Verify target column exists\n",
    "if TARGET_COLUMN not in df.columns:\n",
    "    print(\"CRITICAL ERROR: Target column was lost during processing.\")\n",
    "    raise ValueError(f\"Target column '{TARGET_COLUMN}' not found in processed data.\")\n",
    "\n",
    "# Check that all columns are numeric\n",
    "non_numeric = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(f\"ERROR: Found non-numeric columns: {non_numeric}\")\n",
    "    print(\"These columns could not be converted automatically.\")\n",
    "    raise ValueError(\"Non-numeric columns remain.\")\n",
    "print(\"All columns are numeric.\")\n",
    "\n",
    "# Handle infinite values if present\n",
    "inf_count = np.isinf(df.values).sum()\n",
    "if inf_count > 0:\n",
    "    print(f\"Warning: Found {inf_count} infinite value(s). Replacing with column median.\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "else:\n",
    "    print(\"No infinite values.\")\n",
    "\n",
    "# Fill any remaining missing values (including those created by cleaning steps)\n",
    "for col in df.columns:\n",
    "    if df[col].isna().sum() > 0:\n",
    "        if col == TARGET_COLUMN and PROBLEM_TYPE != 'regression':\n",
    "            # For classification, drop rows with invalid target instead of filling\n",
    "            invalid_rows = df[col].isna().sum()\n",
    "            if invalid_rows > 0:\n",
    "                print(f\"Warning: {invalid_rows} rows had invalid target values (removed).\")\n",
    "                df = df.dropna(subset=[TARGET_COLUMN])\n",
    "        else:\n",
    "            # Fill features with median\n",
    "            median_val = df[col].median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = 0  # Fallback if entire column was empty\n",
    "            df.loc[:, col] = df[col].fillna(median_val)\n",
    "\n",
    "print(\"Missing values handled.\")\n",
    "\n",
    "# Final check for missing values\n",
    "total_missing = df.isna().sum().sum()\n",
    "if total_missing > 0:\n",
    "    print(f\"ERROR: {total_missing} missing values remain.\")\n",
    "    raise ValueError(\"Missing values remain.\")\n",
    "print(\"No missing values.\")\n",
    "\n",
    "# Remove duplicate rows\n",
    "n_duplicates = df.duplicated().sum()\n",
    "if n_duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {n_duplicates} duplicate rows.\")\n",
    "else:\n",
    "    print(\"No duplicate rows.\")\n",
    "\n",
    "# Verify and re-encode target classes if necessary\n",
    "if PROBLEM_TYPE in ['binary_classification', 'multiclass_classification']:\n",
    "    actual_classes = sorted(df[TARGET_COLUMN].unique())\n",
    "    expected_classes = list(range(len(actual_classes)))\n",
    "    \n",
    "    if actual_classes != expected_classes:\n",
    "        print(f\"\")\n",
    "        print(f\"Note: Re-encoding target classes to ensure sequential numbering.\")\n",
    "        print(f\"  Classes present: {actual_classes} -> Re-encoding to: {expected_classes}\")\n",
    "        new_mapping = {old: new for new, old in enumerate(actual_classes)}\n",
    "        df[TARGET_COLUMN] = df[TARGET_COLUMN].map(new_mapping)\n",
    "        \n",
    "        # Update original mapping for reference\n",
    "        if ORIGINAL_CLASS_MAPPING:\n",
    "            inverse_original = {v: k for k, v in ORIGINAL_CLASS_MAPPING.items()}\n",
    "            updated_mapping = {}\n",
    "            for old_encoded, new_encoded in new_mapping.items():\n",
    "                if old_encoded in inverse_original:\n",
    "                    original_label = inverse_original[old_encoded]\n",
    "                    updated_mapping[original_label] = new_encoded\n",
    "            ORIGINAL_CLASS_MAPPING = updated_mapping\n",
    "            print(\"  Updated class mapping.\")\n",
    "        \n",
    "        # Check if problem type shifted (e.g., multiclass -> binary)\n",
    "        if len(actual_classes) == 2 and PROBLEM_TYPE == 'multiclass_classification':\n",
    "            PROBLEM_TYPE = 'binary_classification'\n",
    "            print(f\"  Problem type changed to: binary classification\")\n",
    "        elif len(actual_classes) == 1:\n",
    "            print(\"\")\n",
    "            print(\"!\" * 60)\n",
    "            print(\"ERROR: ONLY ONE CLASS REMAINS\")\n",
    "            print(\"!\" * 60)\n",
    "            print(\"After cleaning, only one target class remains.\")\n",
    "            print(\"A classifier cannot learn from data with only one class.\")\n",
    "            raise ValueError(\"Only one target class remaining.\")\n",
    "\n",
    "# Check for float32 overflow before conversion\n",
    "max_float32 = np.finfo(np.float32).max\n",
    "min_float32 = np.finfo(np.float32).min\n",
    "overflow_cols = []\n",
    "for col in df.columns:\n",
    "    col_max = df[col].max()\n",
    "    col_min = df[col].min()\n",
    "    if col_max > max_float32 or col_min < min_float32:\n",
    "        overflow_cols.append(col)\n",
    "        df[col] = df[col].clip(lower=min_float32, upper=max_float32)\n",
    "\n",
    "if overflow_cols:\n",
    "    print(f\"Warning: Clipped {len(overflow_cols)} column(s) with values exceeding float32 range.\")\n",
    "\n",
    "# Convert all to float32 (required by Linear Learner)\n",
    "df = df.astype(np.float32)\n",
    "print(\"Converted all values to float32.\")\n",
    "\n",
    "# Move target column to first position\n",
    "cols = [TARGET_COLUMN] + [c for c in df.columns if c != TARGET_COLUMN]\n",
    "df = df[cols]\n",
    "print(\"Moved target column to first position.\")\n",
    "\n",
    "# Calculate final dimensions\n",
    "n_features = len(df.columns) - 1\n",
    "n_rows = len(df)\n",
    "\n",
    "# Track duplicates removed for summary\n",
    "ROWS_DROPPED_DUPLICATES = n_duplicates\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Final dataset: {n_rows:,} rows x {len(df.columns)} columns\")\n",
    "print(f\"Features: {n_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 10: Preparation Summary and Recommendation\n",
    "\n",
    "This summary tells you whether your dataset is ready for SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PREPARATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "What was cleaned:\n",
      "  - Removed 1 useless column(s) (IDs, constants)\n",
      "  - Removed 6 column(s) with too many missing values\n",
      "  - Converted 37 text column(s) to numbers\n",
      "\n",
      "Final dataset: 2,930 rows x 227 features\n",
      "Problem type: Regression\n",
      "\n",
      "============================================================\n",
      "RECOMMENDATION\n",
      "============================================================\n",
      "\n",
      ">>> PROCEED <<<\n",
      "\n",
      "Your dataset passed all quality checks and is ready for SageMaker.\n",
      "Continue to Step 11 to export your cleaned file.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PREPARATION COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate total rows dropped\n",
    "ROWS_DROPPED = ROWS_DROPPED_MISSING + ROWS_DROPPED_DUPLICATES\n",
    "\n",
    "# Summarize what was cleaned\n",
    "changes = []\n",
    "if COLS_DROPPED_USELESS > 0:\n",
    "    changes.append(f\"Removed {COLS_DROPPED_USELESS} useless column(s) (IDs, constants)\")\n",
    "if COLS_DROPPED_MISSING > 0:\n",
    "    changes.append(f\"Removed {COLS_DROPPED_MISSING} column(s) with too many missing values\")\n",
    "if COLS_DROPPED_CARDINALITY > 0:\n",
    "    changes.append(f\"Removed {COLS_DROPPED_CARDINALITY} column(s) with too many text categories\")\n",
    "if ROWS_DROPPED > 0:\n",
    "    changes.append(f\"Removed {ROWS_DROPPED:,} row(s) (missing data, duplicates)\")\n",
    "if COLS_ENCODED > 0:\n",
    "    changes.append(f\"Converted {COLS_ENCODED} text column(s) to numbers\")\n",
    "\n",
    "if changes:\n",
    "    print(\"\")\n",
    "    print(\"What was cleaned:\")\n",
    "    for change in changes:\n",
    "        print(f\"  - {change}\")\n",
    "\n",
    "print(\"\")\n",
    "print(f\"Final dataset: {n_rows:,} rows x {n_features} features\")\n",
    "print(f\"Problem type: {PROBLEM_TYPE.replace('_', ' ').title()}\")\n",
    "\n",
    "# Evaluate dataset suitability\n",
    "issues = []\n",
    "\n",
    "# Check 1: Too few rows\n",
    "if n_rows < 300:\n",
    "    issues.append(\"Very few rows remaining (less than 300)\")\n",
    "\n",
    "# Check 2: More features than rows (after encoding)\n",
    "if n_features > n_rows:\n",
    "    issues.append(f\"More columns ({n_features}) than rows ({n_rows:,}) after encoding\")\n",
    "\n",
    "# Check 3: Lost most of the data\n",
    "retention_rate = n_rows / RAW_ROWS\n",
    "if retention_rate < 0.3:\n",
    "    issues.append(f\"More than 70% of original data was removed\")\n",
    "\n",
    "# Check 4: Class imbalance (classification only) - IMPROVED for binary and multiclass\n",
    "if PROBLEM_TYPE != 'regression':\n",
    "    class_distribution = df[TARGET_COLUMN].value_counts(normalize=True).sort_index()\n",
    "    max_class_pct = class_distribution.max()\n",
    "    min_class_pct = class_distribution.min()\n",
    "    n_classes = df[TARGET_COLUMN].nunique()\n",
    "    \n",
    "    if PROBLEM_TYPE == 'binary_classification':\n",
    "        # For binary classification, focus on minority class percentage\n",
    "        if min_class_pct < 0.05:\n",
    "            issues.append(f\"Minority class is only {min_class_pct*100:.1f}% of data (severe imbalance)\")\n",
    "        elif min_class_pct < 0.15:\n",
    "            issues.append(f\"Minority class is only {min_class_pct*100:.1f}% of data (significant imbalance)\")\n",
    "        elif min_class_pct < 0.25:\n",
    "            issues.append(f\"Minority class is {min_class_pct*100:.1f}% of data (moderate imbalance)\")\n",
    "    else:\n",
    "        # For multiclass, check if any single class dominates\n",
    "        expected_balanced_pct = 1.0 / n_classes\n",
    "        if max_class_pct > 0.95:\n",
    "            issues.append(f\"One category represents {max_class_pct*100:.1f}% of the data (severe imbalance)\")\n",
    "        elif max_class_pct > expected_balanced_pct * 4:\n",
    "            issues.append(f\"One category represents {max_class_pct*100:.1f}% of the data (class imbalance)\")\n",
    "    \n",
    "    # Always show distribution for classification problems\n",
    "    print(\"\")\n",
    "    print(\"Target class distribution:\")\n",
    "    for class_val, pct in class_distribution.items():\n",
    "        # Look up original label if available\n",
    "        original_label = class_val\n",
    "        if ORIGINAL_CLASS_MAPPING:\n",
    "            for orig, enc in ORIGINAL_CLASS_MAPPING.items():\n",
    "                if enc == class_val:\n",
    "                    original_label = orig\n",
    "                    break\n",
    "        bar = \"*\" * int(pct * 40)\n",
    "        print(f\"  {original_label}: {pct*100:5.1f}% {bar}\")\n",
    "\n",
    "# Display recommendation\n",
    "print(\"\")\n",
    "print(\"=\" * 60)\n",
    "print(\"RECOMMENDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(issues) == 0:\n",
    "    print(\"\")\n",
    "    print(\">>> PROCEED <<<\")\n",
    "    print(\"\")\n",
    "    print(\"Your dataset passed all quality checks and is ready for SageMaker.\")\n",
    "    print(\"Continue to Step 11 to export your cleaned file.\")\n",
    "    RECOMMENDATION = \"PROCEED\"\n",
    "elif len(issues) == 1:\n",
    "    print(\"\")\n",
    "    print(\">>> PROCEED WITH CAUTION <<<\")\n",
    "    print(\"\")\n",
    "    print(\"Your dataset can be used, but there is one concern:\")\n",
    "    print(f\"  - {issues[0]}\")\n",
    "    print(\"\")\n",
    "    print(\"You may continue to Step 11, but model performance may be limited.\")\n",
    "    print(\"Consider finding a better dataset if results are poor.\")\n",
    "    RECOMMENDATION = \"CAUTION\"\n",
    "else:\n",
    "    print(\"\")\n",
    "    print(\">>> STOP - CONSIDER A DIFFERENT DATASET <<<\")\n",
    "    print(\"\")\n",
    "    print(\"Your dataset has multiple issues that may prevent good results:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "    print(\"\")\n",
    "    print(\"You can still export and try training, but results will likely be poor.\")\n",
    "    print(\"Recommendation: Choose a different dataset or consult your instructor.\")\n",
    "    RECOMMENDATION = \"STOP\"\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## STEP 11: Export Cleaned Data\n",
    "\n",
    "This creates the final CSV file ready for upload to AWS SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FILE EXPORTED\n",
      "============================================================\n",
      "\n",
      "Saved as: AmesHousing_CLEANED.csv\n",
      "\n",
      "File size: 2,930 rows x 228 columns\n",
      "\n",
      "Preview of first 5 rows:\n",
      "215000.0 526301088.0 20.0 141.0 31770.0 0.151195 6.0 5.0 1960.0 1960.0 112.0  639.0   0.0  441.0 1080.0 1656.0   0.0 0.0 1656.0 1.0 0.0 1.0 0.0 3.0 1.0 7.0 2.0 1960.0 2.0 528.0 210.0 62.0 0.0 0.0   0.0 0.0     0.0 5.0 2010.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0\n",
      "105000.0 526350048.0 20.0  80.0 11622.0 0.151195 5.0 6.0 1961.0 1961.0   0.0  468.0 144.0  270.0  882.0  896.0   0.0 0.0  896.0 0.0 0.0 1.0 0.0 2.0 1.0 5.0 0.0 1961.0 1.0 730.0 140.0  0.0 0.0 0.0 120.0 0.0     0.0 6.0 2010.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0\n",
      "172000.0 526351008.0 20.0  81.0 14267.0 0.151195 6.0 6.0 1958.0 1958.0 108.0  923.0   0.0  406.0 1329.0 1329.0   0.0 0.0 1329.0 0.0 0.0 1.0 1.0 3.0 1.0 6.0 0.0 1958.0 1.0 312.0 393.0 36.0 0.0 0.0   0.0 0.0 12500.0 6.0 2010.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0\n",
      "244000.0 526353024.0 20.0  93.0 11160.0 0.151195 7.0 5.0 1968.0 1968.0   0.0 1065.0   0.0 1045.0 2110.0 2110.0   0.0 0.0 2110.0 1.0 0.0 2.0 1.0 3.0 1.0 8.0 2.0 1968.0 2.0 522.0   0.0  0.0 0.0 0.0   0.0 0.0     0.0 4.0 2010.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0\n",
      "189900.0 527105024.0 60.0  74.0 13830.0 0.056314 5.0 5.0 1997.0 1998.0   0.0  791.0   0.0  137.0  928.0  928.0 701.0 0.0 1629.0 0.0 0.0 2.0 1.0 3.0 1.0 6.0 1.0 1997.0 2.0 482.0 212.0 34.0 0.0 0.0   0.0 0.0     0.0 3.0 2010.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 1.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 1.0 0.0\n"
     ]
    }
   ],
   "source": [
    "# Create output filename (FIX: sanitize special characters)\n",
    "base_name = os.path.splitext(FILE_NAME)[0]\n",
    "# Replace spaces and special characters with underscores for S3 compatibility\n",
    "safe_base_name = re.sub(r'[^\\w\\-]', '_', base_name)\n",
    "output_file = f\"{safe_base_name}_CLEANED.csv\"\n",
    "\n",
    "# Save without headers or index (required by Linear Learner)\n",
    "df.to_csv(output_file, index=False, header=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FILE EXPORTED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\")\n",
    "print(f\"Saved as: {output_file}\")\n",
    "print(f\"\")\n",
    "print(f\"File size: {n_rows:,} rows x {len(df.columns)} columns\")\n",
    "print(f\"\")\n",
    "print(\"Preview of first 5 rows:\")\n",
    "print(df.head().to_string(index=False, header=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Troubleshooting\n",
    "\n",
    "**\"Target column not found\"**\n",
    "- Check the spelling of your target column name\n",
    "- Column names are case-sensitive (\"Price\" is different from \"price\")\n",
    "- Check for extra spaces before or after the column name\n",
    "- Look at the list of available columns in the error message\n",
    "\n",
    "**\"Too many categories in text target column\"**\n",
    "- Your target column has text values with too many unique entries\n",
    "- Classification works best with fewer than 20 categories\n",
    "- Consider grouping similar categories together, or use a numeric target for regression\n",
    "\n",
    "**\"Target must have at least 2 unique values\"**\n",
    "- Your target column has only one value (or all values are missing)\n",
    "- A model cannot learn to predict when there is nothing to distinguish\n",
    "- Check that you selected the correct target column\n",
    "\n",
    "**\"Too few rows remaining\"**\n",
    "- Your dataset has too many missing values\n",
    "- Try a different dataset with more complete data\n",
    "\n",
    "**\"More columns than rows\"**\n",
    "- After converting text to numbers, you have too many columns\n",
    "- This happens with datasets that have many text categories\n",
    "- Try a dataset with fewer text columns or more rows\n",
    "\n",
    "**\"Dataset may not work well\"**\n",
    "- The notebook found issues that may prevent good results\n",
    "- You can still try training, but consider using a different dataset\n",
    "\n",
    "**Questions?**\n",
    "- Consult your instructor or TA for help choosing an appropriate dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
